{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules to use for AWS SDK for python\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The user, dwhadmin, was created by AWS console. Dwhadmin has AWS administrator access. This is needed for \n",
    "# security reasons to access resourses on AWS with access key and secret key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters in dwh.cfg\n",
    "# The key and secret were retrieved from the IAM user (dwhadmin) which \n",
    "# has policy as AdministerAccess\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "CLUSTER_TYPE       = config.get(\"CLUSTER\",\"CLUSTER_TYPE\")\n",
    "NUM_NODES          = config.get(\"CLUSTER\",\"NUM_NODES\")\n",
    "NODE_TYPE          = config.get(\"CLUSTER\",\"NODE_TYPE\")\n",
    "\n",
    "CLUSTER_IDENTIFIER = config.get(\"CLUSTER\",\"CLUSTER_IDENTIFIER\")\n",
    "DB_NAME            = config.get(\"DWH\",\"DB_NAME\")\n",
    "DB_USER            = config.get(\"DWH\",\"DB_USER\")\n",
    "DB_PASSWORD        = config.get(\"DWH\",\"DB_PASSWORD\")\n",
    "PORT               = config.get(\"DWH\",\"DB_PORT\")\n",
    "\n",
    "DB_IAM_ROLE_NAME      = config.get(\"CLUSTER\", \"DB_IAM_ROLE_NAME\")\n",
    "ARN                   = config.get(\"IAM_ROLE\", \"ARN\")\n",
    "\n",
    "#(DB_USER, DB_PASSWORD, DB_NAME)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"CLUSTER_TYPE\", \"NUM_NODES\", \"NODE_TYPE\", \"CLUSTER_IDENTIFIER\", \"DB_NAME\", \"DB_USER\", \"DB_PASSWORD\", \"DB_PORT\", \"DB_IAM_ROLE_NAME\", \"ARN\"],\n",
    "              \"Value\":\n",
    "                  [CLUSTER_TYPE, NUM_NODES, NODE_TYPE, CLUSTER_IDENTIFIER, DB_NAME, DB_USER, DB_PASSWORD, PORT, DB_IAM_ROLE_NAME, ARN]\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AWS resources and clients\n",
    "import boto3\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )\n",
    "\n",
    "s3client = boto3.client('s3',\n",
    "                   region_name=\"us-west-2\",\n",
    "                   aws_access_key_id=KEY,\n",
    "                   aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIST json files in s3://udacity-dend/log_data\n",
    "sampleDbBucket_log = s3.Bucket('udacity-dend')\n",
    "for obj_log in sampleDbBucket_log.objects.filter(Prefix='log_data'):\n",
    "    print(obj_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIST contents of s3://udacity-dend/log_json_path.json\n",
    "obj_log_file = s3client.get_object(Bucket='udacity-dend', Key='log_data/2018/11/2018-11-01-events.json')\n",
    "df_log = pd.read_json(obj_log_file['Body'], lines=True)\n",
    "df_log.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIST some json files in s3://udacity-dend/song_data. For examle in /A/A/A subdirectories\n",
    "sampleDbBucket_song = s3.Bucket('udacity-dend')\n",
    "for obj_song in sampleDbBucket_song.objects.filter(Prefix='song_data/A/A/A'):\n",
    "    print(obj_song)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIST contents of s3://udacity-dend/song_data/A/A/A/RAAAAK128F9318786.json\n",
    "obj_song_file = s3client.get_object(Bucket='udacity-dend', Key='song_data/A/A/A/TRAAAAK128F9318786.json')\n",
    "df_song = pd.read_json(obj_song_file['Body'],typ='series')\n",
    "df_song\n",
    "#df_song_frame = df_song.to_frame().reset_index().T\n",
    "#df_song_frame.dtypes\n",
    "#df_song_frame.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### START HERE #####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIST contents of s3://udacity-dend/song_data/A/A/A/TRAAAAK128F9318786.json\n",
    "#obj_song_file = s3client.get_object(Bucket='udacity-dend', Key='song_data/A/A/A/TRAAAAK128F9318786.json')\n",
    "#df_song = pd.read_json(obj_song_file)\n",
    "#df_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIST the s3://udacity-dend/log_json_path.json file  \n",
    "sampleDbBucket_logpath = s3.Bucket('udacity-dend')\n",
    "for obj_logpath in sampleDbBucket_logpath.objects.filter(Prefix='log_json_path.json'):\n",
    "    print(obj_logpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIST contents of s3://udacity-dend/log_json_path.json\n",
    "obj_logpath_file = s3client.get_object(Bucket='udacity-dend', Key='log_json_path.json')\n",
    "df_logpath = pd.read_json(obj_logpath_file['Body'])\n",
    "df_logpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IAM ROLE, dwhRole, was created manually using the aws console. The Role give S3 access to redshift.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Redshift cluster named dwhcluster\n",
    "\n",
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=CLUSTER_TYPE,\n",
    "        NodeType=NODE_TYPE,\n",
    "        NumberOfNodes=int(NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DB_NAME,\n",
    "        ClusterIdentifier=CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DB_USER,\n",
    "        MasterUserPassword=DB_PASSWORD,\n",
    "        \n",
    "        #Roles\n",
    "        IamRoles=['arn:aws:iam::488211246959:role/dwhRole']\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give the status of the cluster . The cluster will eventually become available. \n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the endpoint of the cluster and update HOST in the dwh.cfg file\n",
    "# The cluster should be available before running this code . \n",
    "# Get the role arn \n",
    "DB_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DB_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DB_ENDPOINT :: \", DB_ENDPOINT)\n",
    "print(\"DB_ROLE_ARN :: \", DB_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the tcp port to open connection to cluster\n",
    "# The secrutiy group , sg-ac82dfd1, was created using the AWS console. This security group \n",
    "# will allow any incoming public traffic to connect to the Redshift cluster using TCP port 5439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that you can connect to the cluster , dwhCluster \n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, DB_ENDPOINT, PORT, DB_NAME)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running the sql SELECT queries below run 'create_tables.py' first and then 'etl.py' to create all \n",
    "# staging, fact , and dimension tables and to then load data in these tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from staging_events\n",
    "limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from staging_songs\n",
    "limit 5 ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO users\n",
    "(\n",
    "    SELECT DISTINCT userid as user_id, firstname as first_name, lastname as last_name, gender, level\n",
    "    FROM staging_events\n",
    "    WHERE (staging_events.userid IS NOT NULL AND staging_events.level IS NOT NULL)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM users\n",
    "limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO songs\n",
    "(\n",
    "    SELECT song_id, title, artist_id, year, duration\n",
    "    FROM staging_songs\n",
    "    WHERE (staging_songs.title IS NOT NULL AND staging_songs.artist_id IS NOT NULL AND\n",
    "          staging_songs.year IS NOT NULL AND staging_songs.duration IS NOT NULL)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM songs\n",
    "limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO artists\n",
    "(\n",
    "    SELECT artist_id, artist_name AS name, artist_location AS location,\n",
    "    artist_latitude AS latitude, artist_longtitude AS longtitude\n",
    "    FROM staging_songs\n",
    "    WHERE (staging_songs.artist_name IS NOT NULL AND staging_songs.artist_location IS NOT NULL)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM artists\n",
    "limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO time\n",
    "(\n",
    "  SELECT TIMESTAMP 'epoch' + ts/1000 * interval '1 second' AS start_time, \n",
    "  EXTRACT(HOUR FROM TIMESTAMP 'epoch' + ts/1000 * interval '1 second' ) AS hour,\n",
    "  EXTRACT(DAY FROM TIMESTAMP 'epoch' + ts/1000 * interval '1 second' ) AS day,\n",
    "  EXTRACT(WEEK FROM TIMESTAMP 'epoch' + ts/1000 * interval '1 second' ) AS week,\n",
    "  EXTRACT(MONTH FROM TIMESTAMP 'epoch' + ts/1000 * interval '1 second' ) AS month,\n",
    "  EXTRACT(YEAR FROM TIMESTAMP 'epoch' + ts/1000 * interval '1 second' ) AS year,\n",
    "  EXTRACT(DOW FROM TIMESTAMP 'epoch' + ts/1000 * interval '1 second' ) AS weekday  \n",
    "  FROM staging_events\n",
    "  WHERE staging_events.ts IS NOT NULL  \n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM time\n",
    "limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%sql\n",
    "#INSERT INTO songplays\n",
    "#(start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "#(\n",
    "    #SELECT TIMESTAMP 'epoch' + se.ts/1000 * interval '1 second' AS start_time,\n",
    "    #se.userid AS user_id, \n",
    "    #se.level AS level, \n",
    "    #songs.song_id AS song_id,\n",
    "    #artists.artist_id AS artist_id, \n",
    "    #se.sessionid AS session_id, \n",
    "    #se.location AS location, \n",
    "    #se.useragent AS user_agent\n",
    "    #FROM staging_events se, artists\n",
    "    #LEFT JOIN songs ON songs.artist_id = artists.artist_id\n",
    "    #WHERE (page = 'NextSong' AND start_time IS NOT NULL)\n",
    "#);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM songplays\n",
    "limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check status of cluster\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
